{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Semantic Search Engine using Sentence-BERT (SBERT)","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nIn this notebook, we will build a semantic search engine using the Quora Question Pairs dataset. We will leverage the Sentence-BERT (SBERT) model to generate sentence embeddings and perform efficient similarity searches. The goal is to allow users to input queries and retrieve relevant questions based on their semantic meaning.\n\n## Objectives\n\n- Understand and implement the SBERT model for text embeddings.\n- Develop a semantic search functionality.\n- Evaluate the performance of the search engine.\n## Dataset\n\nWe will use the [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs) dataset, which contains pairs of questions that may be semantically similar.\n","metadata":{}},{"cell_type":"code","source":"### Import Libraries\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom transformers import AutoModel, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport re","metadata":{},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning Function\nThe clean_text function removes unwanted characters and standardizes the text to improve the quality of our embeddings.","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    # Convert text to lowercase\n    text = text.lower()\n    # Remove non-alphabetic characters\n    text = re.sub(r'[^a-z\\s]', '', text)\n    # Replace multiple spaces with a single space\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text  # Return the cleaned text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the Dataset\nWe load the dataset and perform cleaning operations. This includes removing duplicate entries and handling missing values, which helps maintain the integrity of our data.","metadata":{}},{"cell_type":"code","source":"# Load your data from a CSV file\ndata = pd.read_csv('./data/questions.csv')\n\n# Clean the text for both question columns\ndata['question1'] = data['question1'].apply(clean_text)\ndata['question2'] = data['question2'].apply(clean_text)\n\n# Remove duplicate questions and NaN values\ndata.drop_duplicates(inplace=True)\ndata.dropna(inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the SBERT Model","metadata":{}},{"cell_type":"code","source":"# Load the pre-trained Sentence-BERT model\nmodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')","metadata":{},"execution_count":3,"outputs":[{"name":"stderr","output_type":"stream","text":"C:\\Users\\Sajad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n\n  warnings.warn(\n"}]},{"cell_type":"markdown","source":"## Batch Processing for Encoding\nEncoding questions in batches helps manage memory usage and speeds up processing. The encode_questions function generates embeddings for all questions by processing them in smaller groups.","metadata":{}},{"cell_type":"code","source":"# Define batch size for processing\nbatch_size = 32\n\n# Function to encode questions in batches\ndef encode_questions(questions):\n    embeddings = []  # List to store embeddings\n    # Iterate over the questions in batches\n    for i in range(0, len(questions), batch_size):\n        batch = questions[i:i + batch_size].tolist()  # Get a batch of questions\n        # Encode the batch and extend the embeddings list\n        embeddings.extend(model.encode(batch))\n    return np.array(embeddings)  # Return embeddings as a numpy array\n\n# Encode the questions from both columns\n# We use \"set\" function to drop the repeated sentences\nq_embeddings = encode_questions(list(set(list(data['question1']) + list(data['question2']))))\nq_embeddings = normalize(q_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## User Query and Similarity Calculation\nOnce we have the embeddings, we can compute the similarity between a user query and the questions in the dataset using cosine similarity. This metric evaluates how close two vectors are in the embedding space.","metadata":{}},{"cell_type":"code","source":"# Define a user input query\nuser_query = \"What is there\"\n\n# Generate embedding for the user query\nquery_embedding = model.encode(user_query)\nquery_embedding = normalize(query_embedding.reshape(-1,1)).reshape(1,-1)[0]\n\n# Calculate cosine similarities between the query embedding and question2 embeddings\nsimilarities = cosine_similarity([query_embedding], q_embeddings)[0]\n\n# Specify the number of top similar questions to retrieve\ntop_n = 5\n# Get indices of the most similar questions based on cosine similarity\nmost_similar_indices = similarities.argsort()[-top_n*5:][::-1]\n\n# Display the results\nprint(\"Top similar questions:\")\nfor index in most_similar_indices[:top_n]:\n    print(data['question1'].iloc[index])  # Print the most similar questions\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\nIn this notebook, we have successfully implemented a semantic search engine using SBERT on the Quora Question Pairs dataset. By encoding the questions into embeddings, we can retrieve semantically similar questions based on user queries, demonstrating the power of modern natural language processing techniques.\n### Future Work\nExplore Other Models: Investigate the performance of different models for embedding generation.\nAdvanced Ranking Algorithms: Implement sophisticated ranking algorithms to improve search results.\nUser Feedback Loop: Incorporate user feedback to continuously refine and enhance the system.\n## References\n* Sentence Transformers Documentation\n* Quora Question Pairs Dataset","metadata":{}},{"cell_type":"markdown","source":"\n### Instructions for Use:\n\n1. **Copy the Markdown**: Copy the above markdown and paste it into a new Kaggle notebook.\n2. **Add Your Data**: Make sure to upload the `questions.csv` dataset to your Kaggle notebook.\n3. **Run the Cells**: Execute each code cell step by step.\n4. **Save and Share**: Once youâ€™re satisfied, save the notebook and share it on Kaggle. You can also upload it to GitHub.\n\nFeel free to modify any part of the template as needed!\n","metadata":{}}]}