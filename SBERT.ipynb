{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2171098f-6d57-4613-8595-70eea0a1f931",
   "metadata": {},
   "source": [
    "# Semantic Search Engine using Sentence-BERT (SBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fc023-32ab-454a-adbc-e6339b139c15",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will build a semantic search engine using the Quora Question Pairs dataset. We will leverage the Sentence-BERT (SBERT) model to generate sentence embeddings and perform efficient similarity searches. The goal is to allow users to input queries and retrieve relevant questions based on their semantic meaning.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Understand and implement the SBERT model for text embeddings.\n",
    "- Develop a semantic search functionality.\n",
    "- Evaluate the performance of the search engine.\n",
    "## Dataset\n",
    "\n",
    "We will use the [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs) dataset, which contains pairs of questions that may be semantically similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1bf22e5-56c0-438e-baaa-cdd3e78cde8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cae337-90be-45c5-a9ff-9dc70016f182",
   "metadata": {},
   "source": [
    "## Data Cleaning Function\n",
    "The clean_text function removes unwanted characters and standardizes the text to improve the quality of our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f46ae-82b7-472d-ac48-90e56e7a54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text  # Return the cleaned text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc6b25-c1b3-4bf5-8298-b1f93785e935",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "We load the dataset and perform cleaning operations. This includes removing duplicate entries and handling missing values, which helps maintain the integrity of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a27b37-e520-4169-ad07-7ab3d11a600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data from a CSV file\n",
    "data = pd.read_csv('./data/questions.csv')\n",
    "\n",
    "# Clean the text for both question columns\n",
    "data['question1'] = data['question1'].apply(clean_text)\n",
    "data['question2'] = data['question2'].apply(clean_text)\n",
    "\n",
    "# Remove duplicate questions and NaN values\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253503d-1feb-4554-8b0c-4312b2adbaff",
   "metadata": {},
   "source": [
    "## Load the SBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c4ea9d-1723-4fc3-ae31-2e75126e1c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sajad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48da5e-be84-4fdc-b98b-d843228e2882",
   "metadata": {},
   "source": [
    "## Batch Processing for Encoding\n",
    "Encoding questions in batches helps manage memory usage and speeds up processing. The encode_questions function generates embeddings for all questions by processing them in smaller groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4769c5a-cf78-4d59-b740-665feb09a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size for processing\n",
    "batch_size = 32\n",
    "\n",
    "# Function to encode questions in batches\n",
    "def encode_questions(questions):\n",
    "    embeddings = []  # List to store embeddings\n",
    "    # Iterate over the questions in batches\n",
    "    for i in range(0, len(questions), batch_size):\n",
    "        batch = questions[i:i + batch_size].tolist()  # Get a batch of questions\n",
    "        # Encode the batch and extend the embeddings list\n",
    "        embeddings.extend(model.encode(batch))\n",
    "    return np.array(embeddings)  # Return embeddings as a numpy array\n",
    "\n",
    "# Encode the questions from both columns\n",
    "# We use \"set\" function to drop the repeated sentences\n",
    "q_embeddings = encode_questions(list(set(list(data['question1']) + list(data['question2']))))\n",
    "q_embeddings = normalize(q_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ab368-4052-4cf5-a3fb-9a0461874eac",
   "metadata": {},
   "source": [
    "## User Query and Similarity Calculation\n",
    "Once we have the embeddings, we can compute the similarity between a user query and the questions in the dataset using cosine similarity. This metric evaluates how close two vectors are in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f0bb0-6708-447e-9610-31082486539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a user input query\n",
    "user_query = \"What is there\"\n",
    "\n",
    "# Generate embedding for the user query\n",
    "query_embedding = model.encode(user_query)\n",
    "query_embedding = normalize(query_embedding.reshape(-1,1)).reshape(1,-1)[0]\n",
    "\n",
    "# Calculate cosine similarities between the query embedding and question2 embeddings\n",
    "similarities = cosine_similarity([query_embedding], q_embeddings)[0]\n",
    "\n",
    "# Specify the number of top similar questions to retrieve\n",
    "top_n = 5\n",
    "# Get indices of the most similar questions based on cosine similarity\n",
    "most_similar_indices = similarities.argsort()[-top_n*5:][::-1]\n",
    "\n",
    "# Display the results\n",
    "print(\"Top similar questions:\")\n",
    "for index in most_similar_indices[:top_n]:\n",
    "    print(data['question1'].iloc[index])  # Print the most similar questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b03ca78-7086-47ee-bed6-16037cb71df2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we have successfully implemented a semantic search engine using SBERT on the Quora Question Pairs dataset. By encoding the questions into embeddings, we can retrieve semantically similar questions based on user queries, demonstrating the power of modern natural language processing techniques.\n",
    "### Future Work\n",
    "Explore Other Models: Investigate the performance of different models for embedding generation.\n",
    "Advanced Ranking Algorithms: Implement sophisticated ranking algorithms to improve search results.\n",
    "User Feedback Loop: Incorporate user feedback to continuously refine and enhance the system.\n",
    "## References\n",
    "* Sentence Transformers Documentation\n",
    "* Quora Question Pairs Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df95ac5-fcd0-499d-a0e7-8982734e53ce",
   "metadata": {},
   "source": [
    "\n",
    "### Instructions for Use:\n",
    "\n",
    "1. **Copy the Markdown**: Copy the above markdown and paste it into a new Kaggle notebook.\n",
    "2. **Add Your Data**: Make sure to upload the `questions.csv` dataset to your Kaggle notebook.\n",
    "3. **Run the Cells**: Execute each code cell step by step.\n",
    "4. **Save and Share**: Once youâ€™re satisfied, save the notebook and share it on Kaggle. You can also upload it to GitHub.\n",
    "\n",
    "Feel free to modify any part of the template as needed!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
